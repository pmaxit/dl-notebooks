#AUTOGENERATED! DO NOT EDIT! File to edit: dev/01_simpleCNN.ipynb (unless otherwise specified).

__all__ = ['SimpleCNN', 'SimpleCNNWITHBN', 'ConvNet']

#Cell

from fastai2.basics import *
from fastai2.callback.all import *
from fastai2.vision.all import *

from fastai2.data.all import *
from fastai2.vision.core import *
from fastai2.vision.data import *
import matplotlib.pyplot as plt

#Cell
class SimpleCNN(nn.Module):
    def __init__(self, *args, **kwargs):
        """ Basic ConvNet """
        super().__init__(*args, **kwargs)

        # 3 X 32 X 32 -> 64 x 32 x 32
        # first layer always increases the dimensions to large number.
        # usually it involves using larger filter size or successive convolution

        self.conv1 = nn.Conv2d(3, 10, 5 , padding=3)
        # Here padding 1 helps it to keep same size

        self.conv2 = nn.Sequential(
            nn.Conv2d(10, 20, 3, padding=1),
            nn.MaxPool2d(2)
        )

        self.conv3 = nn.Sequential(
            nn.Conv2d(20, 40, 3, padding=1),
            nn.MaxPool2d(2)
        )
        self.conv4 = nn.Sequential(
            nn.Conv2d(40, 60, 3, padding=1),
            nn.MaxPool2d(2)
        )


        self.conv_out = nn.Conv2d(10, 10, 3, padding=1)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))

        x = F.relu(self.conv_out(x))
        x = F.adaptive_avg_pool2d(x, 1)
        x = x.view(x.size(0), -1)
        return F.log_softmax(x,dim=0)


#Cell
class SimpleCNNWITHBN(nn.Module):
    def __init__(self, *args, **kwargs):
        """ Basic ConvNet """
        super().__init__(*args, **kwargs)

        # 3 X 32 X 32 -> 64 x 32 x 32
        # first layer always increases the dimensions to large number.
        # usually it involves using larger filter size or successive convolution

        self.conv1 = nn.Conv2d(3, 10, 5 , padding=3)
        # Here padding 1 helps it to keep same size

        self.conv2 = nn.Sequential(
            nn.Conv2d(10, 20, 3, padding=1),
            nn.BatchNorm2d(20),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )

        self.conv3 = nn.Sequential(
            nn.Conv2d(20, 40, 3, padding=1),
            nn.BatchNorm2d(40),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        self.conv4 = nn.Sequential(
            nn.Conv2d(40, 60, 3, padding=1),
            nn.BatchNorm2d(60),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )


        self.conv_out = nn.Conv2d(60, 10, 3, padding=1)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))

        x = F.relu(self.conv_out(x))
        x = F.adaptive_avg_pool2d(x, 1)
        x = x.view(x.size(0), -1)
        return F.log_softmax(x,dim=0)


#Cell
class ConvNet(nn.Module):
    def __init__(self, layers, c):
        super().__init__()
        self.conv1 = ConvLayer(3, layers[0], ks=5, stride=1, padding=3)
        self.layers = nn.ModuleList([
            ConvLayer(layers[i], layers[i+1],ks=3,stride=2)
        for i in range(len(layers)- 1)])
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.out = nn.Linear(layers[-1],c)

    def forward(self, x):
        x = self.conv1(x)
        for l in self.layers: x = F.relu(l(x))
        x = self.pool(x)
        x = x.view(x.size(0),-1)
        return F.log_softmax(self.out(x), -1)