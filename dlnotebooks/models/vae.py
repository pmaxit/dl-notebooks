#AUTOGENERATED! DO NOT EDIT! File to edit: dev/04_vae.ipynb (unless otherwise specified).

__all__ = ['Encoder', 'Decoder', 'init_weights']

#Cell

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import re

#Cell
class Encoder(nn.Module):


    def __init__(self, imgsz, n_hidden, n_output, keep_prob):
        super(Encoder, self).__init__()

        self.imgsz = imgsz
        self.n_hidden = n_hidden
        self.n_output = n_output
        self.keep_prob = keep_prob

        self.net = nn.Sequential(
            nn.Linear(imgsz, n_hidden),
            nn.ELU(inplace=True),
            nn.Dropout(1-keep_prob),

            nn.Linear(n_hidden, n_hidden),
            nn.Tanh(),
            nn.Dropout(1-keep_prob),

            nn.Linear(n_hidden, n_output*2)

        )

    def forward(self, x):
        """

        :param x:
        :return:
        """
        mu_sigma = self.net(x)


        # The mean parameter is unconstrained
        mean = mu_sigma[:, :self.n_output]
        # The standard deviation must be positive. Parametrize with a softplus and
        # add a small epsilon for numerical stability
        stddev = 1e-6 + F.softplus(mu_sigma[:, self.n_output:])


        return mean, stddev

#Cell
class Decoder(nn.Module):


    def __init__(self, dim_z, n_hidden, n_output, keep_prob):
        super(Decoder, self).__init__()

        self.dim_z = dim_z
        self.n_hidden = n_hidden
        self.n_output = n_output
        self.keep_prob = keep_prob

        self.net = nn.Sequential(
            nn.Linear(dim_z, n_hidden),
            nn.Tanh(),
            nn.Dropout(1-keep_prob),

            nn.Linear(n_hidden, n_hidden),
            nn.ELU(),
            nn.Dropout(1-keep_prob),

            nn.Linear(n_hidden, n_output),
            nn.Sigmoid()
        )

    def forward(self, h):
        """

        :param h:
        :return:
        """
        return self.net(h)


#Cell

def init_weights(encoder, decoder):

    def init_(m):
        with torch.no_grad():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight)
                nn.init.constant_(m.bias, 0.0)

    for m in encoder.modules():
        m.apply(init_)
    for m in decoder.modules():
        m.apply(init_)

    print('weights inited!')