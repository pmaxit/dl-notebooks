{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2 import *\n",
    "from fastai2.text import *\n",
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as sklearn_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing and term document matrix creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast.ai has a number of *datasets hosted via AWS Open datasets* for easy download. We can see them by checking the docs for URLs (remember ?? is a helpful command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://files.fast.ai/data/examples/imdb_sample.tgz'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URLs.IMDB_SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('C:/Users/t158185/.fastai/data/imdb')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [Path('C:/Users/t158185/.fastai/data/imdb/train/labeledBow.feat'),Path('C:/Users/t158185/.fastai/data/imdb/train/neg'),Path('C:/Users/t158185/.fastai/data/imdb/train/pos'),Path('C:/Users/t158185/.fastai/data/imdb/train/unsupBow.feat')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\t158185\\\\Downloads\\\\imdb_sample\\\\imdb_sample\\\\imdb_sample\\\\texts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data processing, we will deal with transformations. transformations are functions which can be applied on the dataset and convert it into required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[attrgetter('label'), Categorize()]\n",
    "tlist = TfmdList(df, tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#10) ['negative','positive','negative','positive','negative','negative','negative','negative','negative','negative']\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlist.decode(tlist[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the same for text. For text we need to follow following procedures\n",
    "1. Tokenize the text\n",
    "2. Add extra tokens to take care of special cases\n",
    "3. Numericalize the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfms = [attrgetter('text'), Tokenizer.from_df('text'), Numericalize(min_freq=3)]\n",
    "tlits = TfmdList(df, tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj un - xxunk - believable ! xxmaj meg xxmaj ryan does n't even look her usual xxunk lovable self in this , which normally makes me forgive her shallow xxunk acting xxunk . xxmaj hard to believe she was the producer on this dog . xxmaj plus xxmaj kevin xxmaj kline : what kind of suicide trip has his career been on ? xxmaj xxunk … xxmaj xxunk xxrep 3 ! xxmaj finally this was directed by the guy who did xxmaj big xxmaj xxunk ? xxmaj must be a replay of xxmaj jonestown - hollywood style . w xxrep 3 o xxrep 3 f !\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlits.decode(tlits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, a token is the basic unit of processing ( what the tokens are depends on the application and choices ). Here the tokens mostly correspond to words or punctuations, as well as other several tokens, corresponding to unknown words, capitalization etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All those tokens starting wtih \"xx\" are fastai special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19240"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tlits.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a datasource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSource is just the fancy name to combine two lists, add splits and convert to train / valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits = RandomSplitter()(range_of(df))\n",
    "dsrc = DataSource(df, tfms=[[attrgetter('text'), Tokenizer.from_df('text'), Numericalize(min_freq=3)],[attrgetter('label'), Categorize()]], splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsrc.train), len(dsrc.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsrc.o2i['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'language'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsrc.vocab[0][938]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Matrices in (Scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though, we've reduced over 19,000 words down to 6,000 that is stil a lot, Most tokens don't appear in most reviews. We want to take advantage of this by storing our data in sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the most common sparse storage formats.\n",
    "* Coordinate wise (scipy calls COO)\n",
    "* Compressed sparse row (CSR)\n",
    "* compressed sparse column (CSC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t158185\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix, csr_matrix, coo_matrix\n",
    "\n",
    "np.random.seed = 0\n",
    "mat = csr_matrix(np.random.random((10, 12))> 0.7, dtype=int)\n",
    "mat[1,0]=2\n",
    "mat[0,1]=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 1 0 1 1 0 1 0 1 1 0]\n",
      " [2 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 1 1 0]\n",
      " [0 0 0 0 1 0 1 0 1 0 1 1]\n",
      " [0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 1 1 1 1]\n",
      " [0 0 0 0 1 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(mat.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse matrix is stored as three dense array, *data*, *indices* and *indptr*\n",
    "- Data contains the non-zero values of the matrix, in the order in which they would be encountered if we walked along the columns top to bottom and left to right. If this wre a csr matrix, the walk would have been along the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 1, 1, 1, 1, 1, 2, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7, 10, 12, 15, 16, 20, 25, 27, 32, 34], dtype=int32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.indptr  # row pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  4,  5,  7,  9, 10,  0,  2, 11,  7, 11,  6,  7, 11,  7,  2,\n",
       "        6,  9, 10,  4,  6,  8, 10, 11,  5,  7,  4,  8,  9, 10, 11,  4,  6],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.indices # column pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mat.indices) # column pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coo_matrix(docs, vocab_len):\n",
    "    \n",
    "    rows=[]\n",
    "    datas = []\n",
    "    cols = []\n",
    "    \n",
    "    for i,d in enumerate(docs):\n",
    "        uniq_indices, counts = np.unique(d[0].numpy(), return_counts=True)\n",
    "        for key,value in zip(uniq_indices, counts):\n",
    "            rows.append(i)     # appending the doc_idx\n",
    "            cols.append(key)      # appending where token is situated\n",
    "            datas.append(value)\n",
    "    \n",
    "    return coo_matrix((datas, (rows,cols)), shape=(len(docs), vocab_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = get_coo_matrix(dsrc.train, len(dsrc.vocab[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    2,    5,    7,    8,    9,   10,   11,   12,   13,   14,\n",
       "          15,   16,   17,   18,   19,   20,   21,   22,   23,   24,   25,\n",
       "          28,   29,   30,   31,   33,   34,   35,   36,   38,   39,   40,\n",
       "          46,   49,   54,   55,   56,   58,   60,   61,   64,   72,   74,\n",
       "          77,   82,   88,   92,   93,  119,  120,  125,  126,  132,  133,\n",
       "         143,  151,  161,  170,  182,  203,  212,  223,  234,  248,  268,\n",
       "         269,  283,  290,  292,  295,  325,  328,  329,  349,  351,  361,\n",
       "         434,  450,  452,  464,  467,  487,  544,  627,  650,  659,  660,\n",
       "         984, 1170, 1399, 1402, 1420, 1561, 1834, 1852, 1944, 2001, 2146,\n",
       "        3135, 4466, 4589, 4853, 5374, 6022, 6505, 6571, 6587], dtype=int64),\n",
       " array([ 6,  1,  2,  3, 18,  5,  3, 12,  3,  5,  4,  3,  4,  2,  3,  2,  2,\n",
       "         3,  4,  1,  1,  1,  2,  4,  2,  1,  3,  2,  1,  2,  1,  1,  1,  2,\n",
       "         1,  1,  1,  1,  1,  2,  1,  2,  1,  1,  1,  2,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  5,  2,\n",
       "         1,  1,  1,  1,  1,  1], dtype=int64))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dsrc.train[0][0].numpy(),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.todense()[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to sparse matrix csr format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_term_doc = dtm.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 7080)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(824, 824)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the 2nd document\n",
    "\n",
    "trn_term_doc[1].sum() , len(dsrc.train[1][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 313)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the unique tokens\n",
    "len(trn_term_doc[1].data), len(torch.unique(dsrc.train[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the log-count ratio r for each word f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trn_term_doc\n",
    "y = np.array([y[1].numpy().item() for y in dsrc.train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.squeeze(np.asarray(x[y==1].sum(0)))\n",
    "p0 = np.squeeze(np.asarray(x[y==0].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5644,    0,  370,    0,    0,   71,    0,  579, 9329, 5166],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times does the word \"loved\" appear in neg vs positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsrc.o2i['loved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 12)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[y==1][:,532].sum(), x[y==0][:,532].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times does the word 'hated' appear in neg vs positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = dsrc.o2i['hated']\n",
    "x[y==1][:,ind].sum(), x[y==0][:,ind].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive reviews with the word hated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.argwhere(x[:,ind]>0)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.argwhere(y==1)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{221}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(a).intersection(set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos xxmaj there are numerous films relating to xxup xxunk , but xxmaj mother xxmaj night is quite distinctive among them : xxmaj in this film , we are introduced to xxmaj howard xxmaj campbell ( nolte ) , an xxmaj american living in xxmaj berlin and married to a xxmaj german , xxmaj xxunk xxmaj xxunk ( lee ) , who decides to accept the role of a spy : xxmaj more specifically , a xxup cia agent xxmaj major xxmaj xxunk ( goodman ) recruits xxmaj campbell who becomes a xxmaj nazi xxunk in order to enter the highest xxunk of the xxmaj hitler xxunk . xxmaj however , the deal is that the xxup us xxmaj government will never xxunk xxmaj campbell \\'s role in the war for national security reasons , and so xxmaj campbell becomes a hated figure across the xxup us . xxmaj after the war , he tries to xxunk his identity , but the past comes back and haunts him . xxmaj his only \" friend \" is xxmaj xxunk , but even he can not do much for the xxunk of events that fall upon poor xxmaj campbell … \\n\\n xxmaj the story is deeply touching , as we watch the tragedy of xxmaj campbell who although a great patriot , is treated by disdain by everybody who xxunk him . xxmaj not only that , but he also gradually realizes that even the persons who are most close to him , have many secrets of their own . xxmaj vonnegut provides us with a moving atmosphere , with xxmaj campbell \\'s despair building up and almost choking the viewer . \\n\\n xxmaj nolte plays the role of his life , in my opinion ; he is even better than in \" xxunk \" , although in both roles he plays tragic figures who are destined to self - destruction . xxmaj xxunk xxmaj lee is also excellent , and the same can be said for the whole cast in general . \\n\\n i have n\\'t read the book , so i can not xxunk how the film compares to it . xxmaj in any case , this is something of no importance here : xxmaj my critique is upon the film per xxunk , and the film xxunk deserves a 9 / 10 .',\n",
       " 'positive')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsrc.decode(dsrc.train[221])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.squeeze(np.array(x[y==1].sum(axis=0)))\n",
    "p2 = np.squeeze(np.array(x[y==1].sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = (p1+1)/((y==1).sum()+1)\n",
    "pr0 = (p2+1)/((y==0).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14990603, 0.14990603, 0.14990603, ..., 0.14990603, 0.14990603,\n",
       "       0.14990603])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.log(pr1/pr0); r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab most likely associated with positive / negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest = np.argpartition(r,-10)[-10:]\n",
    "smallest = np.argpartition(r, 10)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jeffrey',\n",
       " 'crafted',\n",
       " 'president',\n",
       " 'screenwriter',\n",
       " 'portray',\n",
       " 'claiming',\n",
       " 'china',\n",
       " 'eve',\n",
       " 'respective',\n",
       " 'xxfake']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dsrc.vocab[0][k] for k in biggest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['80', 'carol', 'lots', 'up', 'dr', 'dahl', 'davis', 'stick', 'some', 'elvira']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dsrc.vocab[0][k] for k in smallest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_coo_matrix(dsrc.train, len(dsrc.vocab[0]))\n",
    "y_train = np.array([y[1].item() for y in dsrc.train])\n",
    "\n",
    "x_test = get_coo_matrix(dsrc.valid, len(dsrc.vocab[0]))\n",
    "y_test = np.array([y[1].item() for y in dsrc.valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "\n",
    "m.fit(x_train, y_train)\n",
    "y_pred = m.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred==y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.squeeze(m.coef_)\n",
    "\n",
    "biggest = np.argpartition(arr, -10)[-10:]\n",
    "smallest = np.argpartition(arr, 10)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['also',\n",
       " 'both',\n",
       " 'best',\n",
       " 'more',\n",
       " 'very',\n",
       " 'may',\n",
       " 'well',\n",
       " 'see',\n",
       " 'great',\n",
       " 'times']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dsrc.vocab[0][k] for k in biggest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', '*', '?', 'worst', 'were', 'no', 'plot', 'simply', 'poor', 'acting']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dsrc.vocab[0][k] for k in smallest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MultinomialNB()\n",
    "m.fit(x_train.toarray(), y_train)\n",
    "y_pred = m.predict(x_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred==y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.squeeze(m.coef_)\n",
    "\n",
    "biggest = np.argpartition(arr, -10)[-10:]\n",
    "smallest = np.argpartition(arr, 10)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'to', 'of', 'a', 'and', 'xxmaj', 'the', ',', '.', 'xxunk']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dsrc.vocab[0][k] for k in biggest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hallan',\n",
       " 'dave',\n",
       " 'bollywood',\n",
       " 'sheets',\n",
       " 'balcony',\n",
       " 'flies',\n",
       " 'unwisely',\n",
       " 'pokes',\n",
       " 'laying',\n",
       " 'object']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dsrc.vocab[0][k] for k in smallest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('base': conda)",
   "language": "python",
   "name": "python37164bitbaseconda387944bc4e8e4a1c87919da3be7aca06"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
